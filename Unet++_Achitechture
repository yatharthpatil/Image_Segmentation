from keras.src.layers.attention.multi_head_attention import activation

# this is the residual block of our Unet. here we perform two convolution operation and three dilated convolution with rate 3,5,7.and batchnormalizatin
def residual_block(width):
  def apply(x):
    size = x.shape[3]
    if(size == width):
      residual = x
    else:
      residual = layers.Conv2D(width, kernel_size=1)(x)

    x = layers.BatchNormalization(center = False, scale = False) (x)
    x = layers.Conv2D(width, kernel_size= 3 , padding='same',activation= 'relu')(x)
    l1 = layers.Conv2D(width, kernel_size = 3, padding='same', dilation_rate = 3,activation = 'relu')(x)
    l2 = layers.Conv2D(width, kernel_size = 3, padding = 'same', dilation_rate = 5,activation = 'relu')(x)
    l3 = layers.Conv2D(width, kernel_size = 3, padding = 'same', dilation_rate = 7,activation = 'relu')(x)
    x = layers.Conv2D(width, kernel_size= 3 , padding='same')(x)
    x = layers.Add()([x,residual,l1,l2,l3])
    return x
  return apply

# this forms decoder part of the Unet

def decoder_block(width, block_depth):
  def apply(x):
    x , skips = x
    x = layers.UpSampling2D(size = 2 , interpolation = 'bilinear')(x)

    for _ in range(block_depth):
      x = layers.Concatenate()([x,skips.pop()])
      x = residual_block(width)(x)

    return x

  return apply

# this forms encoder part of our Unet

def encoder_block(width,block_depth):
  def apply(x):
    x,skips = x

    for _ in range(block_depth):
      x = residual_block(width)(x)
      skips.append(x)

    x = layers.AveragePooling2D(pool_size = 2)(x)
    return x
  return apply


# here we are generating all the skip connection for the unet++

def skip_final(skips):
  skips_true = []
  temp = []
  skips_true.append(skips[0])
  count = 0
  while(True):
    if count%2 == 0:

      for i in range(len(skips)-1):
        if i%2 == 0:
          temp.append(layers.Concatenate()([skips[i+1],skips[i]]))
        else:
          temp.append(layers.Concatenate()([skips[i+1],layers.UpSampling2D(size = 2, interpolation = 'bilinear')(skips[i])]))
      # print(layers.Concatenate()([skips[i+1],layers.UpSampling2D(size = 2, interpolation = 'bilinear')(skips[i])]).shape)
    else:

      for i in range(len(skips)-1):
        if i%2 == 0:
          temp.append(layers.Concatenate()([skips[i+1],layers.UpSampling2D(size = 2, interpolation = 'bilinear')(skips[i])]))

        else:
          temp.append(layers.Concatenate()([skips[i+1],skips[i]]))

    count = count + 1


    skips = temp
    skips_true.append(temp[0])
    if(len(temp) == 1):
      break
    temp = []

  return skips_true


def Network(image_h,image_w, widths, block_depth):

  image = keras.Input(shape = (image_h,image_w))
  image = tf.expand_dims(image,axis = 3)

  x = layers.Conv2D(widths[0],kernel_size=1)(image)
  skips = []
  for width in widths[:-1]:
    x = encoder_block(width,block_depth)([x,skips])


  for _ in range(block_depth):
    x = residual_block(widths[-1])(x)

# here we are generating skip connections
  skips.reverse()
  skips = skip_final(skips)
  skips.reverse()

# here i am adding a new layer at the bottle neck, after finding that results depend on the filters present at the bottle neck
  x_0 = layers.Conv2D(512, kernel_size = 1, padding='same',activation = 'relu')(x)
  x_1 = layers.Conv2D(512, kernel_size = 3, padding='same',activation = 'relu')(x)
  x_2 = layers.Conv2D(512, kernel_size = 5, padding='same',activation = 'relu')(x)
  x_3 = layers.Conv2D(512, kernel_size = 7, padding='same',activation = 'relu')(x)
  x = layers.Add()([x_0,x_1,x_2,x_3,x])

# from here upsampling is starting

  for width in reversed(widths[:-1]):
    x = decoder_block(width,block_depth)([x,skips])


  x = layers.Conv2D(2,kernel_size = 3,padding = 'same' ,activation= 'softmax' )(x)
  # x = tf.argmax(x,axis = -1)


  return keras.Model(image,x,name = "residual_unet")


abc = Network(image_h,image_w,widths,block_depth)
abc.summary()
