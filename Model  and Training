class image_segment(keras.Model):


  def __init__(self,image_h,image_w,widths,block_depth):
    super().__init__()

    self.normalizer = layers.Normalization()
    self.network = Network(image_h,image_w,widths,block_depth)
    self.ema_network = keras.models.clone_model(self.network)


  def compile(self,**kwargs):
    super().compile(**kwargs)
    # self.dice = dice(name = "dice_track")
    self.loss_tracker = keras.metrics.Mean(name = "t_loss")

  @property
  def metric(self):
    return [self.loss_tracker]


  def train_model(self,image,training):
    if training:
      network = self.network
    else:
      network = self.ema_network

    pred_image = network(image,training = training)
    return pred_image

# this is our training step. here we normalize our image and then pass it to the unet++ for forwward pass. then gradients are calculated. the optimizer RMSprop is used for backpropagation.

  def train_step(self,data):
    image, mask = data
    image = self.normalizer(image, training = True)

    with tf.GradientTape() as tape:
      pred_image = self.train_model(image,training = True)
      pred_image = tf.reshape(pred_image,[128,128,2])
      image_loss = self.compiled_loss(mask,pred_image)

    gradients = tape.gradient(image_loss,self.network.trainable_weights)
    self.optimizer.apply_gradients(zip(gradients,self.network.trainable_weights))

    self.loss_tracker.update_state(image_loss)
    # self.dice.update_state(pred_image,mask)

    for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):
      ema_weight.assign(ema*ema_weight + (1-ema)*weight)

    return {m.name: m.result() for m in self.metrics}

# this is the validation step

  def test_step(self,data):
    image,mask = data
    image = self.normalizer(image, training = True)

    pred_image = self.train_model(image,training = False)
    pred_image = tf.reshape(pred_image,[128,128,2])

    image_loss = self.compiled_loss(mask,pred_image)

    self.loss_tracker.update_state(image_loss)

    # self.dice.update_state(pred_image,mask)
    return {m.name: m.result() for m in self.metrics}



# here we will state the optimizer and loss function

# here we convert the imput list to tensors, as input to the model is in the form of tensors and not list

train_image = tf.convert_to_tensor(train_image)
train_mask = tf.convert_to_tensor(train_mask)
val_image = tf.convert_to_tensor(val_image)
val_mask = tf.convert_to_tensor(val_mask)

print(train_mask.shape)

# Here we are compiling and training the model. we are using sparse categoical crossentropy as loss.

from tensorflow.keras.optimizers import RMSprop

# rmsprop = RMSprop(learning_rate=1e-7, rho=0.9)
learning_rate = 1e-6
weight_decay = 1e-4
model = image_segment(image_h,image_w,widths,block_depth)

model.compile(
        optimizer=keras.optimizers.experimental.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay),
    loss="sparse_categorical_crossentropy", metrics=["accuracy"],
)

checkpoint_path = "/content/drive/MyDrive/DL_pro/"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=True,
    save_best_only=True,
)

# model.normalizer.adapt(train_dataset)

# run training and plot generated images periodically

model.fit(
    train_image,
    train_mask,
    epochs=epoch,
    validation_data=(val_image, val_mask),
    callbacks=[checkpoint_callback],
    batch_size=batch_size,  # Set batch_size to match your dataset's batch size
)

